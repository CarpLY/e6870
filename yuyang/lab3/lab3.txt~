
########################################################################
#   Lab 3: Language Modeling Fever
#   EECS E6870: Speech Recognition
#   Due: March 11, 2016 at 6pm
########################################################################

* Name:Yang Yu (yy2623)


########################################################################
#   Part 1
########################################################################

* Create the file "p1b.counts" by running "lab3_p1b.sh".
  Submit the file "p1b.counts" by typing
  the following command (in the directory ~/e6870/lab3/):

      submit-e6870.py lab3 p1b.counts

  More generally, the usage of "submit-e6870.py" is as follows:

      submit-e6870.py <lab#> <file1> <file2> <file3> ...

  You can submit a file multiple times; later submissions
  will overwrite earlier ones.  Submissions will fail
  if the destination directory for you has not been created
  for the given <lab#>; contact us if this happens.


* Can you name a word/token that will have a different unigram count
  when counted as a history (i.e., histories are immediately to the
  left of a word that we wish to predict the probability of) and
  when counted as a regular unigram (i.e., exactly in a position that we
  wish to predict the probability of)?

-> Token </s>. It only have a regular unigram count but doesn't have a history 
unigram count. Because no word or token will appear after token </s>.



 *Sometimes in practice, the same token is used as both a
  beginning-of-sentence marker and end-of-sentence marker.
  In this scenario, why is it a bad idea to count these markers
  at the beginnings of sentences (in addition to at the ends of
  sentences) when computing regular unigram counts?

->first of all, the number of a beginning-of-sentence marker may vary
 according to the value of n in n-gram algorithm. Thus, such counting 
is not stable and reasonable.Then, Counting the beginnings of sentences 
will increase the possiblility of the end-of-sentence marker. Such increase 
will cause the future sentence stops early which doesn't show any meaningful
effect.


* In this lab, we update counts for each sentence separately, which
  precludes the updating of n-grams that span two different sentences.
  A different strategy is to concatenate all of the training sentences
  into one long word sequence (separated by the end-of-sentence token, say)
  and to update counts for all n-grams in this long sequence.  In this
  method, we can update counts for n-grams that span different sentences.
  Is this a reasonable thing to do?  Why or why not?

-> Yes, this is a resonable thing to do.  Then we can also predict the start
of a sentece from the end of the last sentence.


########################################################################
#   Part 2
########################################################################

* Create the file "p2b.probs" by running "lab3_p2b.sh".
  Submit the following files:

      submit-e6870.py lab3 p2b.probs


########################################################################
#   Part 3
########################################################################

* Describe a situation where P_MLE() will be the dominant term
  in the Witten-Bell smoothing equation (e.g., describe how
  many different words follow the given history, and how many
  times each word follows the history):

-> For example, when trainning unigram by running lab3_p3a.sh.
For zero-gram, we have 1655 words for its history and 357 different
words for this given history. Generally speaking, each word will appear
4 or 5 times follow this history. By calculating, the coefficient of P_MLE()
will be 82.26% which means P_MLE() becomes the dominant tern in WB smoothing
equation. Generally speaking, the more times each word appears after a history,
the greater weight P_MLE() will get in WB smoothing equation.


* Describe a situation where P_backoff() will be the dominant term
  in the Witten-Bell smoothing equation:

-> If the given distribution has no history count, then p_MLE() of this
distribution doesn't have meanning and will be ignored. Therefore, P_backoff()
will become its dominant term.


* Create the file "p3b.probs" by running "lab3_p3b.sh".
  Submit the following files:

      submit-e6870.py lab3 lang_model.C p3b.probs


########################################################################
#   Part 4
########################################################################

* What word-error rates did you find for the following conditions?
  (Examine the file "p4.out" to extract this information.)

->
1-gram model (WB; full training set):  30.43%
2-gram model (WB; full training set):  28.60%
3-gram model (WB; full training set):  27.54%

MLE (3-gram; full training set): 29.29%
plus-delta (3-gram; full training set):  29.37%
Witten-Bell (3-gram; full training set): 27.54%

2000 sentences training (3-gram; WB): 29.21%
20000 sentences training (3-gram; WB): 29.52%
200000 sentences training (3-gram; WB): 28.53%


* What did you learn in this part?

-> 1 By increasing value of n in n-gram model, the
recognition accuracy can be increased.
   2 WB smoothing algorithm behaves better than no-smoothing
or plus-delta smoothing given current experiment.
   3 LM tranning need large quantity of trainning data, if 
the data size is not big enough, the accuracy can not be guaranteed.


* When calculating perplexity we need to normalize by the
  number of words in the data.  When counting the number
  of words for computing PP, one convention is
  to include the end-of-sentence token in this count.
  For example, we would count the sentence "WHO IS THE MAN"
  as five words rather than four.  Why might this be a good
  idea?

-> Because when we calculating the probability of current sentence,
we have added a end token into the sentence, so that the probability 
of the last real word can be calculated for different n-gram model.
Therefore, the real number of probabilities mutiplied one-by-one when 
calculating PP should be larger than the length of the sentence. Thus, 
to normalize such probabilities should use one plus its original length.

* If the sentence "OH I CAN IMAGINE" has the following trigram probabilities:

      P(OH | <s> <s>) = 0.063983
      P(I | <s> OH) = 0.126221
      P(CAN | OH I) = 0.006285
      P(IMAGINE | I CAN) = 0.000010
      P(</s> | CAN IMAGINE) = 0.030753

  (<s> is the beginning-of-sentence marker, </s> is the end-of-sentence marker)

  what is the perplexity of this sentence (using the convention mentioned
  in the last question)?

-> 144.98


* For a given task, we can vary the vocabulary size; with smaller
  vocabularies, more words will be mapped to the unknown token.
  Can you say anything about how PP will vary with vocabulary size,
  given the same training and test set (and method for constructing
  the LM)?

-> With smaller vocabularies, more words will be mapped to <UNK>. Therefore
the estimation probability of <UNK> will just increase, and more and more
calculations will be related to <UNK> probability, which means more probabilities
will increase, so that PP will decrease instead. In conclusion, as the vocabulary
diction size decreases, PP will also decrease.



* When handling silence, one option is to treat it as a "transparent" word
  as in the lab.  Another way is to not treat it specially, i.e., to treat it
  as just another word in the vocabulary.  What are some
  advantages/disadvantages of each of these approaches?

->


########################################################################
#   Wrap Up
########################################################################

After filling in all of the fields in this file, submit this file
using the following command:

    submit-e6870.py lab3 lab3.txt

The timestamp on the last submission of this file (if you submit
multiple times) will be the one used to determine your official
submission time (e.g., for figuring out if you handed in the
assignment on time).

To verify whether your files were submitted correctly, you can
use the command:

    check-e6870.py lab3

This will list all of the files in your submission directory,
along with file sizes and submission times.


########################################################################
#
########################################################################


